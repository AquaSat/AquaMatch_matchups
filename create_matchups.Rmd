---
title: "AquaMatch WQP Matchups"
author: "ROSSyndicate"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  markdown: 
    wrap: 80
---

```{r file-setup}
library(tidyverse) 
library(data.table)
# FYI some conflicts with data.table, mostly {dplyr} and {lubridate} 
# functions, to use the tidyverse version of these functions you will 
# need to use the `package_name::function()` (eg: `dplyr::select()`) to
# access
library(sf)
library(sfheaders)
library(arrow)
library(googledrive)
library(tictoc)
library(scales)
library(tigris)

# Files sourced from Drive are available to any Google User, the console will walk through authentication steps.
drive_auth()
```

# Purpose

This markdown file walks through the process of creating matchups from the
AquaMatch harmonized data from the Water Quality Portal and the AquaMatch Landsat C2
SR data across the United States and its Territories.

## Bring in data

### *In situ* data
We access AquaMatch *in situ* data directly from the EDI Data Repository. We adapt
helper scripts generated by EDI to read in the data - these are stored in the `src`
folder of the matchup repository. **Note**: We've edited the helper scripts to keep
the `harmonized_local_time` column in string format, rather than datetime format,
because each row in the local time column is intended to have its own corresponding
time zone specified in the `harmonized_tz` column. R requires a single time zone for
datetime colums. Also, these scripts are based on specific versions of the published
data products. If revised versions are published then the scripts may need to be
updated in order to grab the newest data versions. The code chunk below defines which
published parameters (Chlorophyll *a*, Dissolved Organic Carbon, Secchi Disk Depth,
Total Suspended Solids, etc.) to download from EDI.

```{r download-edi}
# Where to put the data
edi_download_path <- "in/edi_data/"

# Check for download directory, make if it does not exist
if (!dir.exists(edi_download_path)) {
  dir.create(edi_download_path, recursive = TRUE)
}

# This code chunk will take a few minutes to run if not run previously

# Chla
# Citation: 
# Brousil, M.R., M.F. Meyer, K. Willi, B.G. Steele, J. De La Torre, and M.R. Ross. 2024. AquaMatch Chlorophyll a Data from Water Quality Portal: ~1970-2024 ver 2. Environmental Data Initiative. https://doi.org/10.6073/pasta/2f750544112e5408928dd9a61e6ace30 (Accessed 2025-09-17).
source(file = "src/retrieve_chla.R")

# DOC
# Brousil, M.R., M.F. Meyer, K. Willi, B.G. Steele, and M.R. Ross. 2024. AquaMatch Dissolved Organic Carbon Data from Water Quality Portal: ~1970-2024 ver 1. Environmental Data Initiative. https://doi.org/10.6073/pasta/31d271897f074da990a4f25108ff2a40 (Accessed 2025-09-17).
source(file = "src/retrieve_doc.R")

# SDD
# De La Torre, J., B.G. Steele, M.R. Brousil, M.F. Meyer, K. Willi, and M.R. Ross. 2025. AquaMatch Secchi Disk Depth Data from Water Quality Portal: ~1970-2024 ver 1. Environmental Data Initiative. https://doi.org/10.6073/pasta/542a305e8484ae5abc33881bd7761308 (Accessed 2025-09-17).
source(file = "src/retrieve_sdd.R")

# TSS
# Brousil, M.R., M.F. Meyer, K. Willi, B.G. Steele, J.R. Gardner, and M.R. Ross. 2025. AquaMatch Total Suspended Solids from Water Quality Portal ~1970-2025 ver 2. Environmental Data Initiative. https://doi.org/10.6073/pasta/6cde7f8613ef5e7e2bef3a19160e02ea (Accessed 2025-09-17).
source(file = "src/retrieve_tss.R")

all_in_situ <- bind_rows(wqp_chla, wqp_doc, wqp_sdd, wqp_tss)

rm(wqp_chla, wqp_doc, wqp_sdd, wqp_tss)
gc()
```

### Remote sensing data
Next we download the siteSR data. This is currently done from Google Drive, but
in the future this will change once the data are formally published.
```{r download-rs}
rs_download_path <- "in/siteSR_data/"

# Check for download directory, make if it does not exist
if (!dir.exists(rs_download_path)) {
  dir.create(rs_download_path, recursive = TRUE)
}

# At this time, the C2 data are sourced from Drive, will be updated once the
# pub is out.
# This may take several minutes if the files have not yet been downloaded.
# Landsat 4-9 paths are loaded in place of the files themselves due to their size. 
source("src/load_Landsat_C2_SRST.R")

# Confirm that site location data were loaded
dim(site_locs)
```


## Making Matchups

There are many different ways and filters that can be applied to matchups. River
sites change more rapidly than lake sites (generally speaking), so matchup
windows (the time allowed between satellite acquisition and in situ
observation/measurement) should be shorter. We use data.table functions for
these applications because they are the most efficient for datasets of this
size.

The following function makes matchups based on the prescribed window:

```{r define-matchup-function}
#' @param sat_fp: Vector of filepaths to the satellite data
#' @param wqp_data: in situ data 
#' @param site_info Data frame of site metadata containing loc_id and siteSR_id cols
#' @param window: Integer of number of days on either side of the insitu measuerment
make_matchups <- function(sat_fp, wqp_data, site_info, window) {
  
  # Create min and max times within WQP data corresponding to specified window
  wqp_data <- wqp_data %>%
    mutate(
      min_time = ActivityStartDate - days(window),
      max_time = ActivityStartDate + days(window)
    )
  
  # Add siteSR_id to WQP data to allow join with siteSR
  wqp_w_ids <- wqp_data %>%
    left_join(x = .,
              y = select(site_info, loc_id, siteSR_id),
              by = c("MonitoringLocationIdentifier" = "loc_id"))
  
  # Iterate over 
  wqp_matchups <- map(
    .x = sat_fp,
    .f = ~{
      temp_file <- read_feather(file = .x) %>%
        filter(siteSR_id %in% unique(wqp_w_ids$siteSR_id)) %>%
        # This duplicate column will be sacrificed during the join below
        mutate(join_date = date)
      
      # Time range join with data.table
      matchup <- data.table(wqp_w_ids)[data.table(temp_file),
                                       on = .(siteSR_id,
                                              max_time >= join_date,
                                              min_time <= join_date),
                                       nomatch = NULL] %>%
        as_tibble() %>%
        # Calc time difference between reported in situ time and overpass time
        mutate(time_diff = ActivityStartDate - date)
      
      # Clean up
      rm(temp_file)
      gc()
      
      return(matchup)
    }
  ) %>%
    # Stack matchups across all missions 
    bind_rows()
  
  # Return to user with addition of col indicating datetime of (in situ - overpass)
  wqp_matchups
}
```

Perform the matchups:
```{r make-matchups}
# Time the process with {tictoc}
tic()
siteSR_matchups <- make_matchups(
  sat_fp = c(LS4_fp, LS5_fp, LS7_fp, LS8_fp, LS9_fp),
  wqp_data = all_in_situ, 
  site_info = site_locs,
  window = 5
)
toc()

```

We'll remove large unnecessary objects before doing any further calculations:
```{r cleanup}
rm(all_in_situ, site_locs)
gc()
```

## Visualize matchups

Next we'll do some visual checks of the resulting data. First we'll clean up the
geospatial data needed for mapping. Most of the records have WGS84 datum, but a
few don't. How many will we miss if we drop the non-WGS84? Should be very few:

```{r check-datum}
siteSR_matchups %>%
  filter(datum != "WGS84") %>%
  nrow()
```


We can move ahead to plotting the geographic record distribution. Here they are for broken down by the `parameter` column within the conterminous U.S.:
```{r make-param-maps}
state_selection <- states(progress_bar = FALSE) %>%
  st_transform(crs = 9311)

matchups_sf <- siteSR_matchups %>%
  select(parameter, lat, lon, datum) %>%
  filter(datum == "WGS84") %>%
  st_as_sf(crs = 4326, coords = c("lon", "lat")) %>%
  st_transform(crs = 9311)

# Conterminous US sf object
conterminous_us <- tigris::states(progress_bar = FALSE) %>%
  st_transform(crs = 9311) %>%
  filter(!(NAME %in% c("Alaska", "Hawaii", "American Samoa",
                       "Guam", "Puerto Rico",
                       "United States Virgin Islands",
                       "Commonwealth of the Northern Mariana Islands")))

# Other US territories sf object
non_conterminous_us <- tigris::states(progress_bar = FALSE) %>%
  st_transform(crs = 9311) %>%
  filter((NAME %in% c("Alaska", "Hawaii", "American Samoa",
                      "Guam", "Puerto Rico",
                      "United States Virgin Islands",
                      "Commonwealth of the Northern Mariana Islands")))

# Focal records for the map
trim_recs <- matchups_sf[conterminous_us, ] %>%
  sf_to_df(fill = TRUE) 

trim_recs %>%
  ggplot() +
  geom_hex(aes(x = x, y = y)) +
  geom_sf(data = conterminous_us,
          color = "black",
          fill = NA) +
  # facet_grid(rows = vars(parameter)) +
  facet_wrap(vars(parameter)) +
  xlab(NULL) +
  ylab(NULL) +
  scale_fill_viridis_c("Record count",
                       trans = "log",
                       breaks = breaks_log(n = 6),
                       labels = label_number(big.mark = ",")) +
  ggtitle(
    label = "Matchup counts across the US by parameter",
    subtitle = paste0(
      "Not shown: ",
      comma(nrow(matchups_sf[non_conterminous_us,])),
      " records from outside the conterminous US"
    )
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.key.width = unit(1, "cm")
  )


# Clean up the objects after using them
rm(conterminous_us, non_conterminous_us, trim_recs)
gc()
```

And here is a plot of all records, not limited to the conterminous U.S.:
```{r make-full-map}
matchups_sf %>%
  sf_to_df(fill = TRUE) %>%
  ggplot() +
  geom_hex(aes(x = x, y = y)) +
  geom_sf(data = state_selection,
          color = "black",
          fill = NA) +
  xlab(NULL) +
  ylab(NULL) +
  scale_fill_viridis_c("Record count",
                       trans = "log",
                       breaks = breaks_log(n = 6),
                       labels = label_number(big.mark = ",")) +
  ggtitle(label = "Matchup counts") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    legend.key.width = unit(1.25, "cm")
  )

gc()
```


## Applying correction coefficients

Apply the correction coefficients from lakeSR here. They will also eventually be
in the data pub, but for now, grab from the lakeSR repo. One application for Roy
(MLE) and one application for Gardner-style (poly). Also flag input reflectance
beyond the input min/max values listed in the table. See implementation example
in <https://github.com/steeleb/regional-clarity-RS-model/> (not on main repo
yet) in the script `regional_clarity/3_make_models.R`. Note this example does
not flag refelctance beyond min/max of input, but should probably be included in
our data pub, there's an explanation as to why it's important for Gardner model
especially in the bookdown. I'm pretty sure I have code for that somewhere if
it's helpful.

```{r apply-handoffs}
# Placeholder for handoff coefficients
```

Export the final product:
```{r export}
# Where to export the matchup data
output_path <- "out/"

# Check for download directory, make if it does not exist
if (!dir.exists(output_path)) {
  dir.create(output_path)
}

# Export matchups
write_feather(x = siteSR_matchups,
              sink = file.path(output_path, "siteSR_matchups.feather"))
```