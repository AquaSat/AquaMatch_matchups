---
title: "AquaMatch WQP Matchups"
author: "ROSSyndicate"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  markdown: 
    wrap: 80
---

```{r file-setup}
library(tidyverse) 
library(data.table)
# FYI some conflicts with data.table, mostly {dplyr} and {lubridate} 
# functions, to use the tidyverse version of these functions you will 
# need to use the `package_name::function()` (eg: `dplyr::select()`) to
# access
library(sf)
library(arrow)
library(googledrive)
library(tictoc)

# Files sourced from Drive are available to any Google User, the console will walk through authentication steps.
drive_auth()
```

# Purpose

This markdown file walks through the process of creating matchups from the
AquaMatch harmonized data from the Water Quality Portal and the AquaMatch Landsat C2 SR data across the United States and its Territories.

## Bring in data

### *In situ* data
We access AquaMatch *in situ* data directly from the EDI Data Repository. We adapt helper scripts generated by EDI to read in the data - these are stored in the `src` folder of the matchup repository. **Note**: We've edited the helper scripts to keep the `harmonized_local_time` column in string format, rather than datetime format, because each row in the local time column is intended to have its own corresponding time zone specified in the `harmonized_tz` column. R requires a single time zone for datetime colums. Also, these scripts are based on specific versions of the published data products. If revised versions are published then the scripts may need to be updated in order to grab the newest data versions. The code chunk below defines which published parameters (Chlorophyll *a*, Dissolved Organic Carbon, Secchi Disk Depth, Total Suspended Solids, etc.) to download from EDI.

```{r download-edi}
# Where to put the data
edi_download_path <- "in/edi_data/"

# Check for download directory, make if it does not exist
if (!dir.exists(edi_download_path)) {
  dir.create(edi_download_path, recursive = TRUE)
}

# This code chunk will take a few minutes to run if not run previously

# Chla
# Citation: 
# Brousil, M.R., M.F. Meyer, K. Willi, B.G. Steele, J. De La Torre, and M.R. Ross. 2024. AquaMatch Chlorophyll a Data from Water Quality Portal: ~1970-2024 ver 2. Environmental Data Initiative. https://doi.org/10.6073/pasta/2f750544112e5408928dd9a61e6ace30 (Accessed 2025-09-17).
source(file = "src/retrieve_chla.R")

# DOC
# Brousil, M.R., M.F. Meyer, K. Willi, B.G. Steele, and M.R. Ross. 2024. AquaMatch Dissolved Organic Carbon Data from Water Quality Portal: ~1970-2024 ver 1. Environmental Data Initiative. https://doi.org/10.6073/pasta/31d271897f074da990a4f25108ff2a40 (Accessed 2025-09-17).
source(file = "src/retrieve_doc.R")

# SDD
# De La Torre, J., B.G. Steele, M.R. Brousil, M.F. Meyer, K. Willi, and M.R. Ross. 2025. AquaMatch Secchi Disk Depth Data from Water Quality Portal: ~1970-2024 ver 1. Environmental Data Initiative. https://doi.org/10.6073/pasta/542a305e8484ae5abc33881bd7761308 (Accessed 2025-09-17).
source(file = "src/retrieve_sdd.R")

# TSS
# Brousil, M.R., M.F. Meyer, K. Willi, B.G. Steele, J.R. Gardner, and M.R. Ross. 2025. AquaMatch Total Suspended Solids from Water Quality Portal ~1970-2025 ver 2. Environmental Data Initiative. https://doi.org/10.6073/pasta/6cde7f8613ef5e7e2bef3a19160e02ea (Accessed 2025-09-17).
source(file = "src/retrieve_tss.R")

all_in_situ <- bind_rows(wqp_chla, wqp_doc, wqp_sdd, wqp_tss)

rm(wqp_chla, wqp_doc, wqp_sdd, wqp_tss)
gc()
```

### Remote sensing data
Next we download the siteSR data. This is currently done from Google Drive, but in the future this will change once the data are formally published.
```{r download-rs}
rs_download_path <- "in/siteSR_data/"

# Check for download directory, make if it does not exist
if (!dir.exists(rs_download_path)) {
  dir.create(rs_download_path, recursive = TRUE)
}

# At this time, the C2 data are sourced from Drive, will be updated once the
# pub is out.
# This may take several minutes if the files have not yet been downloaded.
# Landsat 4-9 paths are loaded in place of the files themselves due to their size. 
source("src/load_Landsat_C2_SRST.R")

# Confirm that site location data were loaded
dim(site_locs)
```


## Making Matchups

There are many different ways and filters that can be applied to matchups. River
sites change more rapidly than lake sites (generally speaking), so matchup
windows (the time allowed between satellite acquisition and in situ
observation/measurement) should be shorter. We use data.table functions for
these applications because they are the most efficient for datasets of this
size.

The following function makes matchups based on the prescribed window:

```{r define-matchup-function}
#' @param sat_fp: Vector of filepaths to the satellite data
#' @param wqp_data: in situ data 
#' @param site_info Data frame of site metadata containing loc_id and siteSR_id cols
#' @param window: Integer of number of days on either side of the insitu measuerment
make_matchups <- function(sat_fp, wqp_data, site_info, window) {
  
  # Create min and max times within WQP data corresponding to specified window
  wqp_data <- wqp_data %>% 
    mutate(
      min_time = harmonized_utc - days(window),
      max_time = harmonized_utc + days(window)
    )
  
  # Add siteSR_id to WQP data to allow join with siteSR
  wqp_w_ids <- wqp_data %>%
    left_join(x = .,
              y = select(site_info, loc_id, siteSR_id),
              by = c("MonitoringLocationIdentifier" = "loc_id"))
  
  # Iterate over 
  wqp_matchups <- map(
    .x = sat_fp,
    .f = ~{
      temp_file <- read_feather(file = .x) %>%
        filter(siteSR_id %in% unique(wqp_w_ids$siteSR_id)) %>%
        mutate(utc_time = ymd_hms(paste(date, "12:00:00"), tz = "UTC"),
               # This duplicate column will be sacrified during the join below
               utc_time2 = utc_time)
      
      # Time range join with data.table
      matchup <- data.table(wqp_w_ids)[data.table(temp_file),
                                       on = .(siteSR_id,
                                              max_time >= utc_time2,
                                              min_time <= utc_time2),
                                       nomatch = NULL] %>%
        as_tibble() %>%
        # Calc time difference between reported in situ time and overpass time
        mutate(time_diff = round(
          difftime(harmonized_utc, utc_time, units = "hours"),
          digits = 1)
        )
      
      # Clean up
      rm(temp_file)
      gc()
      
      return(matchup)
    }
  ) %>%
    # Stack matchups across all missions 
    bind_rows()
  
  # Return to user with addition of col indicating datetime of (in situ - overpass)
  wqp_matchups
}
```

Perform the matchups:
```{r make-matchups}
# Time the process with tictoc
tic()
siteSR_matchups <- make_matchups(
  sat_fp = c(LS4_fp, LS5_fp, LS7_fp, LS8_fp, LS9_fp),
  wqp_data = all_in_situ, 
  site_info = site_locs,
  window = 5
)
toc()

```

## Applying correction coefficients

Apply the correction coefficients from lakeSR here. They will also eventually be
in the data pub, but for now, grab from the lakeSR repo. One applicaiton for Roy
(MLE) and one application for Gardner-style (poly). Also flag input reflectance
beyond the input min/max values listed in the table. See implementation example
in <https://github.com/steeleb/regional-clarity-RS-model/> (not on main repo
yet) in the script `regional_clarity/3_make_models.R`. Note this example does
not flag refelctance beyond min/max of input, but should probably be included in
our data pub, there's an explanation as to why it's important for Gardner model
especially in the bookdown. I'm pretty sure I have code for that somewhere if
it's helpful.

```{r apply-handoffs}
# Placeholder for handoff coefficients
```

Export the final product:
```{r export}
# Where to export the matchup data
output_path <- "out/"

# Check for download directory, make if it does not exist
if (!dir.exists(output_path)) {
  dir.create(output_path)
}

# Export matchups
write_feather(x = siteSR_matchups,
              sink = file.path(output_path, "siteSR_matchups.feather"))
```